---
title: "Missing Data"
author: "Junyan Yao"
date: "2020-01-22"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Outline of this article
1. Missing Data Mechanisms
2. Simple missing data fixes: LVCF, mean imputation, dummy variable methods
3. More complicated missing data fixes: Weighting, hotdecking, regression imputation
4. Building blocks and overview of multiple imputation, including regresion imputation with noise
5. More advanced imputation and other missing data methods

## Examining Patterns of Missing Data
### Cases:

* Univariate Nonresponse:
![Univariate Nonresponse](/Users/YaoJunyan/Documents/pic/a.jpeg)
* Multivariate Two Patterns:
![Multovariate Two Patterns](/Users/YaoJunyan/Documents/pic/b.png)
* Monotone:
![Monotone](/Users/YaoJunyan/Documents/pic/c.png)
* General:
![General](/Users/YaoJunyan/Documents/pic/d.png)



** Notation and explanation
Let R be the matrix of variables $R_{1}$, $R_{2}$,...,$R_{p}$, corresponding to variables in dataset, $Y_{1}$, $Y_{2}$, ...,$Y_{p}$, that indicate whether a given value of the corresponding Y variable is observed(=1) or missing(=0)

## Missing Data Mechanisms:
* Missing Completely at Random (MCAR) 
    * $P(R_{1}, R_{2}, ..., R_{p}|Y_{1}, Y_{2},...,Y_{p}) = P(R_{1}, R_{2}, ..., R_{p})$
* Missing at Random (MAR)
    * $P(R_{1},...,R_{p}| Y_{1},...,Y_{p})= P(R_{1},...,R_{p}| Y^{obs}_{1},...,Y^{obs}_{p})$ Missingness depends on observed values of the variables
* Not Missing at Random(NMAR), also called Missing Not at Random
    * $$P(R_{1},...,R_{p}| Y_{1},...,Y_{p}) \neq P(R_{1},...,R_{p}| Y^{obs}_{1},...,Y^{obs}_{p})$ Missingness depends on the values of the items that are missing!

MCAR and MAR are both ignorable missing data mechanisms. The term ignorable reflects that fact that for these missing data mechanisms we can make inferences using our data without having to include a model for the missing data mechanism within our analysis model.
NMAR is a non-ignorable missing data mechanism,

### Simple missing data methods:
#### Methods that throw away data
* Complete cases (listwise deletion) ![complete case](/Users/YaoJunyan/Documents/pic/complete_case.png)
* Complete variables (available case) ![complete_var](/Users/YaoJunyan/Documents/pic/complete_var.png)

#### Methods that don't throw away data
* Mean imputation ![mean imputation](/Users/YaoJunyan/Documents/pic/mean_imputation.png)
* Last value carried forward ![LVCF](/Users/YaoJunyan/Documents/pic/LVCF.png)
* Dummy variable method
    * For example, for each predictor variable with missing data, fills in missing values with 0 or the mean and then includes a new variable that is an indicator for missingness for the original variable with missing data.
* Reports from others
    * For example, we are missing data regarding the fathers of children in a dataset, we can probably fill these values in with mother's report of the values.

### More complicated missing data methods

#### Methods that throw away data
* Non-response weighting
    * Suppose only one variable has missing data, we can build a model to predict whether a vlaue is observed using observed values from the other variables. Then use these predicted probabilities to create survey weights of the form $1/P(\frac{R_{i}}{X_{i}})$ to make the complete case sample representative of the full sample once again. Typically we normalize by multiplying the weights by the overall (marginal) probability of missingness, $P(R_{i})$. This way the weights will sum to the number of people left in the complete case sample.
![weighting](/Users/YaoJunyan/Documents/pic/weighting.png)

#### Methods that don't throw away data
* Hotdecking
    * Replaces missing values using other values found in the dataset.For example, for each person with a missing value on variable Y, find another person who has all the same values (or close to the same values) on observed variables $X_{1}, X_{2},X_{3}...$, and use that person's Y value.
    
* Regression imputation
    * Suppose only one variable has missing data, within the complete case sample, build a model that predicts the values of that variable. 
    ![regression](/Users/YaoJunyan/Documents/pic/regression.png)

### Multiple regression imputation with noise

Multiple imputation uses observed data to impute missing values that reflect both sampling variability and model uncertainty. It creates several imputations for each missing value and thus creates several completed datasets.

However, it can be difficult to implement when there are complex patterns of missing data;

```
noise = rnorm(length(y.imp),0,summary(mod)$sigma))
y.imps = y.imp + noise
```

Stochastically impute for binary data:

```
mod =  glm(y~ pred1 + pred2, data = dat.cc, family = 'binomial')
ps = predict(mod, newdata= dat.dropped)
y.imps = rbinom(sum(Ry == 0 ), 1, ps)

```

Stochastically impute for unordered categoricals

```
library(nnet)
mmod = multinom(y ~ pred1 + pred2, data =  dat.cc)
ps = predict(mmod, type = 'prob', newdata = dat.dropped)

```

Assume ps is a matrix of probabilities obtained from fitted model combined with predictor data for observations you want to impute for

```
k = 5  # k is the number of categories
cat.imps = numeric(nimp)
# nimp is the number to be imputed (sum(Ry == 0))

for (i in 1: nimp) {
cat.imps[i] = sum(rmultinom(1,1,ps[i,]) * c(1:k))
}
```
The part within the parentheses will create a vector of 0's except the one category where the draw occurred and that slot will now show its category number rather than a 1. So when you sum all you get back is that category number.

MI Strengths:
* Maintains entire dataset
* Uses all available information
* Weak (more plausible) assumptions about the missing data mechanism
* Properly reflects two kinds of uncertainty about the missing data (so, confidence intervals have correct coverage properties)
    * Sampling uncertainty
    * Model uncertainty
* Maintains relationships between variables
* One set of imputed datasets can be used for many analyses (allowing for release, for example, of public use imputed datasets)

MI Weaknesses:
* Can be more complex to implement (though with current software this is becoming less and less of an issue)
* Have to rely on modeling assumptions

How does it work?
1. Specify a model for the complete data, and fit
2. Use this model to predict missing values
3. Repeat 1-2 M times to create M complete datasets
4. Perform your desired analysis in each dataset

Implementation in R:
```{r}
 library(mi)
# load the data
data(nlsyV)

# Create a missing_data object, look at the data and the missing data pattern
mdf = missing_data.frame(nlsyV)

image(mdf)

hist(mdf)


# Examine the default choices for imputation models
show(mdf)


# Make changes to imputation models if necessary
mdf <- change(mdf, y = c('momed', 'momrace'), what = 'type', to = 'un') # changed ordered to unordered
show(mdf)
# Impute until converged
imputations <- mi(mdf)
converged <- mi2BUGS(imputations)

plot(converged)
# Plot diagnostics
Rhats(imputations) # some values are greater than 1.1

# Iterate bewtween step 4-6 if necessary
mdf2 <- change(mdf, y = 'income', what = 'type', to ='nonn')
imputations2 <- mi(mdf2, n.iter = 60)
Rhats(imputations2) # much better
# Run final pooled analysis
analysis <- pool(ppvtr.36 ~ first + b.marr + scale(income) + momage + momed + momrace, imputations2, m = 5)
print(analysis)

# Now compare to complete case analysis

glm(formula = ppvtr.36 ~ first+ b.marr+ scale(income) + momage + factor(momed) + factor(momrace), family = gaussian, data = nlsyV)


```